{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66551da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# === Configuration ===\n",
    "DATA_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "label = input(\"Book\").strip().lower()\n",
    "SAVE_PATH = os.path.join(DATA_DIR, f\"{label}.csv\")\n",
    "\n",
    "# === MediaPipe setup ===\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2, \n",
    "    min_detection_confidence=0.7\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "data = []\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "\n",
    "print(\"ğŸ“· Starting capture in 3 seconds...\")\n",
    "time.sleep(3)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, c = frame.shape\n",
    "    frame_count += 1\n",
    "\n",
    "    # Detect hands\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # === If no hand detected ===\n",
    "    if not results.multi_hand_landmarks:\n",
    "        cv2.putText(frame, \"No hands detected!\", (10, 35),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Two-Hand Capture\", frame)\n",
    "\n",
    "        # Skip saving this frame (no data collected)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    # === If hand(s) detected, record landmarks ===\n",
    "    row = []\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            row.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "    # Pad second hand if only one hand detected\n",
    "    if len(results.multi_hand_landmarks) == 1:\n",
    "        row.extend([0] * (21 * 3))\n",
    "\n",
    "    row.append(label)\n",
    "    data.append(row)\n",
    "    saved_count += 1\n",
    "\n",
    "    # Draw hands on frame\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.putText(frame, f\"Collecting: {label}\", (10, 35),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Samples: {saved_count}\", (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Two-Hand Capture\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# === Save dataset ===\n",
    "columns = []\n",
    "for hand in [\"L1_\", \"L2_\"]:\n",
    "    for i in range(21):\n",
    "        columns += [f\"{hand}x{i}\", f\"{hand}y{i}\", f\"{hand}z{i}\"]\n",
    "columns.append(\"label\")\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "if not df.empty:\n",
    "    df.to_csv(SAVE_PATH, index=False)\n",
    "    print(f\"\\nâœ… Dataset saved to {SAVE_PATH}\")\n",
    "    print(f\"ğŸ§® Frames processed: {frame_count}\")\n",
    "    print(f\"ğŸ’¾ Valid samples collected: {saved_count}\")\n",
    "else:\n",
    "    print(\"ğŸš« No valid samples collected (no hands detected). File not saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbd432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Found 15 dataset files\n",
      "ğŸ“ Found 15 dataset files\n",
      "Files: ['bahay.csv', 'basahin.csv', 'hindi.csv', 'kamusta.csv', 'kotse.csv', 'libro.csv', 'mahalkita.csv', 'masaya.csv', 'oo.csv', 'paano.csv', 'pagod.csv', 'pamilya.csv', 'salamat.csv', 'telepono.csv', 'tulog.csv']\n",
      "âœ… Loaded bahay.csv (254 samples)\n",
      "âœ… Loaded basahin.csv (563 samples)\n",
      "âœ… Loaded hindi.csv (604 samples)\n",
      "âœ… Loaded kamusta.csv (173 samples)\n",
      "âœ… Loaded kotse.csv (232 samples)\n",
      "âœ… Loaded libro.csv (609 samples)\n",
      "âœ… Loaded mahalkita.csv (214 samples)\n",
      "âœ… Loaded masaya.csv (282 samples)\n",
      "âœ… Loaded oo.csv (273 samples)\n",
      "âœ… Loaded paano.csv (298 samples)\n",
      "âœ… Loaded pagod.csv (216 samples)\n",
      "âœ… Loaded pamilya.csv (271 samples)\n",
      "âœ… Loaded salamat.csv (230 samples)\n",
      "âœ… Loaded telepono.csv (302 samples)\n",
      "âœ… Loaded tulog.csv (552 samples)\n",
      "\n",
      "âœ… Combined dataset created successfully!\n",
      "ğŸ“„ Saved to: C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLdataset.csv\n",
      "ğŸ§® Total samples: 5073\n",
      "ğŸ·ï¸ Unique labels: ['bahay' 'basahin' 'hindi' 'kamusta' 'kotse' 'libro' 'mahalkita' 'masaya'\n",
      " 'oo' 'paano' 'pagod' 'pamilya' 'salamat' 'telepono' 'tulog']\n",
      "\n",
      "ğŸ”§ Cleaning and preparing data...\n",
      "ğŸ’¾ Saved label classes for later decoding\n",
      "\n",
      "ğŸ§  Building TensorFlow Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">975</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m16,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             â”‚           \u001b[38;5;34m975\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,487</span> (99.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,487\u001b[0m (99.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,487</span> (99.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,487\u001b[0m (99.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training model...\n",
      "Epoch 1/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.3256 - loss: 2.1486 - val_accuracy: 0.4446 - val_loss: 1.8250\n",
      "Epoch 2/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4945 - loss: 1.6023 - val_accuracy: 0.5616 - val_loss: 1.4249\n",
      "Epoch 3/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6038 - loss: 1.2589 - val_accuracy: 0.6823 - val_loss: 1.1039\n",
      "Epoch 4/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6839 - loss: 0.9988 - val_accuracy: 0.7069 - val_loss: 0.8696\n",
      "Epoch 5/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7369 - loss: 0.8216 - val_accuracy: 0.7968 - val_loss: 0.7078\n",
      "Epoch 6/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7994 - loss: 0.6570 - val_accuracy: 0.8091 - val_loss: 0.5976\n",
      "Epoch 7/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8182 - loss: 0.5743 - val_accuracy: 0.8621 - val_loss: 0.5009\n",
      "Epoch 8/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8469 - loss: 0.5048 - val_accuracy: 0.8744 - val_loss: 0.4384\n",
      "Epoch 9/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8589 - loss: 0.4614 - val_accuracy: 0.9163 - val_loss: 0.3684\n",
      "Epoch 10/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8758 - loss: 0.4154 - val_accuracy: 0.8670 - val_loss: 0.3633\n",
      "Epoch 11/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8820 - loss: 0.3837 - val_accuracy: 0.9089 - val_loss: 0.3102\n",
      "Epoch 12/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8956 - loss: 0.3324 - val_accuracy: 0.8978 - val_loss: 0.2814\n",
      "Epoch 13/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.3064 - val_accuracy: 0.9360 - val_loss: 0.2481\n",
      "Epoch 14/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9079 - loss: 0.2806 - val_accuracy: 0.9360 - val_loss: 0.2320\n",
      "Epoch 15/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9190 - loss: 0.2629 - val_accuracy: 0.9397 - val_loss: 0.2067\n",
      "Epoch 16/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9184 - loss: 0.2536 - val_accuracy: 0.9200 - val_loss: 0.2111\n",
      "Epoch 17/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9254 - loss: 0.2320 - val_accuracy: 0.9310 - val_loss: 0.1895\n",
      "Epoch 18/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9331 - loss: 0.2202 - val_accuracy: 0.9532 - val_loss: 0.1598\n",
      "Epoch 19/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9316 - loss: 0.2069 - val_accuracy: 0.9495 - val_loss: 0.1683\n",
      "Epoch 20/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9322 - loss: 0.2076 - val_accuracy: 0.9680 - val_loss: 0.1403\n",
      "Epoch 21/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9415 - loss: 0.1816 - val_accuracy: 0.9631 - val_loss: 0.1360\n",
      "Epoch 22/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9473 - loss: 0.1783 - val_accuracy: 0.9667 - val_loss: 0.1336\n",
      "Epoch 23/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9486 - loss: 0.1693 - val_accuracy: 0.9298 - val_loss: 0.1646\n",
      "Epoch 24/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9418 - loss: 0.1718 - val_accuracy: 0.9655 - val_loss: 0.1208\n",
      "Epoch 25/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9513 - loss: 0.1541 - val_accuracy: 0.9766 - val_loss: 0.1099\n",
      "Epoch 26/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9556 - loss: 0.1464 - val_accuracy: 0.9692 - val_loss: 0.1055\n",
      "Epoch 27/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9590 - loss: 0.1391 - val_accuracy: 0.9840 - val_loss: 0.0856\n",
      "Epoch 28/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9572 - loss: 0.1366 - val_accuracy: 0.9643 - val_loss: 0.1030\n",
      "Epoch 29/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9621 - loss: 0.1277 - val_accuracy: 0.9741 - val_loss: 0.0949\n",
      "Epoch 30/30\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9636 - loss: 0.1222 - val_accuracy: 0.9754 - val_loss: 0.0841\n",
      "\n",
      "ğŸ“Š Evaluating model...\n",
      "âœ… Test Accuracy: 0.9704\n",
      "ğŸ“‰ Test Loss: 0.1078\n",
      "ğŸ’¾ Saved Keras model â†’ C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLDatasetModels\\FSL_Dataset.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLDatasetModels\\FSL_Dataset_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLDatasetModels\\FSL_Dataset_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLDatasetModels\\FSL_Dataset_SavedModel'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 15), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1805039031632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039032592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039033936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039031824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039032976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039034512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "ğŸ’¾ Saved TensorFlow SavedModel â†’ C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLDatasetModels\\FSL_Dataset_SavedModel\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\JAMJAY~1\\AppData\\Local\\Temp\\tmpu8glwj7d\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\JAMJAY~1\\AppData\\Local\\Temp\\tmpu8glwj7d\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\JAMJAY~1\\AppData\\Local\\Temp\\tmpu8glwj7d'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 15), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1805039031632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039032592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039033936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039031824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039032976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1805039034512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "ğŸ’¾ Saved TFLite model â†’ C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\\FSLDatasetModels\\FSL_Dataset_Model.tflite\n",
      "\n",
      "âœ… All models exported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ASL Basic Phrases â†’ TensorFlow Deep Learning Version\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# === 1ï¸âƒ£ Dataset Loading & Combining ===\n",
    "DATA_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\FSL\"\n",
    "all_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "\n",
    "print(f\"ğŸ“ Found {len(all_files)} dataset files\")\n",
    "\n",
    "# Load ALL .csv files inside the ASL/FSL folder\n",
    "all_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "\n",
    "print(f\"ğŸ“ Found {len(all_files)} dataset files\")\n",
    "print(\"Files:\", [os.path.basename(f) for f in all_files])\n",
    "\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if df.empty:\n",
    "            print(f\"âš ï¸ Skipped empty file: {os.path.basename(file)}\")\n",
    "            continue\n",
    "\n",
    "        # Add label automatically based on filename\n",
    "        df['label'] = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "        df_list.append(df)\n",
    "        print(f\"âœ… Loaded {os.path.basename(file)} ({df.shape[0]} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {os.path.basename(file)}: {e}\")\n",
    "\n",
    "# Combine all\n",
    "if not df_list:\n",
    "    raise ValueError(\"ğŸš« No valid datasets found. Make sure CSV files are present.\")\n",
    "\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "combined_path = os.path.join(DATA_DIR, \"FSLdataset.csv\")\n",
    "final_df.to_csv(combined_path, index=False)\n",
    "\n",
    "print(\"\\nâœ… Combined dataset created successfully!\")\n",
    "print(f\"ğŸ“„ Saved to: {combined_path}\")\n",
    "print(\"ğŸ§® Total samples:\", final_df.shape[0])\n",
    "print(\"ğŸ·ï¸ Unique labels:\", final_df['label'].unique())\n",
    "\n",
    "\n",
    "# === 2ï¸âƒ£ Preprocessing ===\n",
    "print(\"\\nğŸ”§ Cleaning and preparing data...\")\n",
    "\n",
    "# Drop NaNs and ensure numeric\n",
    "final_df = final_df.dropna()\n",
    "X = final_df.drop('label', axis=1)\n",
    "X = X.apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(final_df['label'])\n",
    "\n",
    "# Save label encoder for later decoding\n",
    "np.save(os.path.join(DATA_DIR, \"ObjectThings_classes.npy\"), label_encoder.classes_)\n",
    "print(\"ğŸ’¾ Saved label classes for later decoding\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === 3ï¸âƒ£ Build TensorFlow Model ===\n",
    "print(\"\\nğŸ§  Building TensorFlow Model...\")\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# === 4ï¸âƒ£ Train Model ===\n",
    "print(\"\\nğŸš€ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === 5ï¸âƒ£ Evaluate ===\n",
    "print(\"\\nğŸ“Š Evaluating model...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"ğŸ“‰ Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# === 6ï¸âƒ£ Save Models (Keras, SavedModel, TFLite) ===\n",
    "TFMODELS_DIR = os.path.join(DATA_DIR, \"FSLDatasetModels\")\n",
    "os.makedirs(TFMODELS_DIR, exist_ok=True)\n",
    "\n",
    "KERAS_PATH = os.path.join(TFMODELS_DIR, \"FSL_Dataset.keras\")\n",
    "SAVEDMODEL_PATH = os.path.join(TFMODELS_DIR, \"FSL_Dataset_SavedModel\")\n",
    "TFLITE_PATH = os.path.join(TFMODELS_DIR, \"FSL_Dataset_Model.tflite\")\n",
    "\n",
    "# Save .keras\n",
    "model.save(KERAS_PATH)\n",
    "print(f\"ğŸ’¾ Saved Keras model â†’ {KERAS_PATH}\")\n",
    "\n",
    "# Save as TensorFlow SavedModel\n",
    "model.export(SAVEDMODEL_PATH)\n",
    "print(f\"ğŸ’¾ Saved TensorFlow SavedModel â†’ {SAVEDMODEL_PATH}\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_PATH, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ğŸ’¾ Saved TFLite model â†’ {TFLITE_PATH}\")\n",
    "\n",
    "print(\"\\nâœ… All models exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba67774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading TensorFlow model...\n",
      "âœ… Loaded model with 15 output labels\n",
      "ğŸ¥ Starting webcam... Press 'q' to quit.\n",
      "ğŸ– Show both hands clearly to the camera.\n",
      "ğŸ›‘ Webcam closed.\n"
     ]
    }
   ],
   "source": [
    "# Real-Time ASL Phrase Prediction (Two-Hand Version) using TensorFlow\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# === Paths ===\n",
    "MODEL_PATH = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\models\\ASLDatasetModels\\ASL_Dataset.keras\"\n",
    "LABEL_PATH = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\SignLanguageRecognition\\dataset\\ASL\\ASL_Dataset_Classes.npy\"\n",
    "\n",
    "# === Load model and labels ===\n",
    "print(\"ğŸ“¦ Loading TensorFlow model...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "label_classes = np.load(LABEL_PATH, allow_pickle=True)\n",
    "print(f\"âœ… Loaded model with {len(label_classes)} output labels\")\n",
    "\n",
    "# === MediaPipe Setup ===\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,                 # âœ… Allow both hands\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# === Helper: Extract both-hand keypoints (126 features = 2 Ã— 21 Ã— 3) ===\n",
    "def extract_two_hand_keypoints(results):\n",
    "    left_hand = np.zeros(21 * 3)\n",
    "    right_hand = np.zeros(21 * 3)\n",
    "\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            label = results.multi_handedness[hand_idx].classification[0].label\n",
    "            coords = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                coords.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "            if label.lower() == 'left':\n",
    "                left_hand = np.array(coords)\n",
    "            else:\n",
    "                right_hand = np.array(coords)\n",
    "\n",
    "    # Always return fixed-length 126-dim vector\n",
    "    return np.concatenate([left_hand, right_hand])\n",
    "\n",
    "# === Prediction Smoothing ===\n",
    "predictions_queue = deque(maxlen=10)\n",
    "\n",
    "# === Start Webcam ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"ğŸš« Cannot access webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Starting webcam... Press 'q' to quit.\")\n",
    "print(\"ğŸ– Show both hands clearly to the camera.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"âš ï¸ Frame capture failed, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Flip and preprocess\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    # === Extract hand features (always 126 features) ===\n",
    "    features = extract_two_hand_keypoints(results).reshape(1, -1)\n",
    "\n",
    "    # === Predict if at least one hand is detected ===\n",
    "    if np.any(features):\n",
    "        probs = model.predict(features, verbose=0)\n",
    "        pred_idx = np.argmax(probs)\n",
    "        pred_label = label_classes[pred_idx]\n",
    "        confidence = probs[0][pred_idx]\n",
    "\n",
    "        predictions_queue.append(pred_label)\n",
    "        stable_prediction = max(set(predictions_queue), key=predictions_queue.count)\n",
    "\n",
    "        # Display text\n",
    "        cv2.putText(frame,\n",
    "                    f\"{stable_prediction} ({confidence*100:.1f}%)\",\n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "                    (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"No Hands Detected\",\n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.0, (0, 0, 255), 2)\n",
    "\n",
    "    # === Draw landmarks for both hands ===\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0),\n",
    "                                       thickness=2, circle_radius=3),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0),\n",
    "                                       thickness=2)\n",
    "            )\n",
    "\n",
    "    # Show live frame\n",
    "    cv2.imshow(\"ASL Phrase Recognition (TensorFlow Two-Hand)\", frame)\n",
    "\n",
    "    # Exit condition\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# === Cleanup ===\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n",
    "print(\"ğŸ›‘ Webcam closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
